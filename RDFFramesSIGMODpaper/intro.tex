%!TEX root = main.tex

% Paragraph1: introducing the RDF data and the need to analyze them. Main ideas:
%1- many graph datasets are encoded in the RDF model 2- it has been integrated in many artificial intelligence applications (both traditional machine learning and relational learning models) so we need to facilitate data science and machine learning on top of them.
There has been a sharp growth in the number of graph datasets that are made available using the RDF\footnote{\url{https://www.w3.org/RDF}} (Resource Description Framework) data model.
Examples include knowledge graphs that cover a broad set of domains such as DBpedia~\cite{lehmann2015dbpedia}, YAGO~\cite{yago2}, and Wikidata~\cite{vrandecic12wikidata}, as well as specialized graphs for specific domains like social media networks, protein interaction networks, and bibliographic datasets. These datasets have been deployed in artificial intelligence applications including recommendation systems, search systems, virtual assistants and question answering. 

% Paragraph 2: to make it accessible for machine learning and data analysis, a lot of data querying and cleaning has to be done to convert it to the tidy data format. the tidy data format makes in an easy and efficient way to process data starting form a standard especially that RDF data is more heterogeneous, incomplete and noisy compared to relational tables
The RDF model provided a powerful abstraction for representing heterogeneous, noisy and incomplete data  which are characteristics of most real world datasets especially knowledge graphs that are automatically extracted from unreliable sources. However, it has complicated the already challenging and time consuming task of cleaning and preparing the data for analysis. It has been reported that $80\%$ of data analysis time and effort is spent on the process of exploring, cleaning and preparing the data.  \cite{dasu2003exploratory}. % I want to add a figure to represent the steps for analyzing the datasets in general. 1- data collection which is done in already in some specialized datasets and our API can be used to do it by navigating knowledge graphs. 
%2- exploring the data (exploratory analysis) and feature selection: finding the number of unique values for each attribute, finding the distribution of the values, finding the correlation between one attribute and the predicted feature or clustering. This helps us in finding the most wieldy target parameter to predict and the features that are fruteful and then querying it to extract the relevant information
%3- structuring it in tidy data format so that data cleaning can be done using data science frameworks
% 4- data cleaning
% 5- prepossessing: splitting into train valid and test, tokenization (converting things to integers), making sure input is of the same length, removing duplicates  
% 5- Analysis, visualization, Model Development and Evaluation
The difficulty can be attributed to the diverse and wide tasks that need to be done as depicted in Figure 1 which represents a typical data science task. The data scientist first has to collect the data from multiple resources to build a dataset, then do some exploratory analysis to find the main types of instances, their attributes, analyze data distribution, define the most feasible prediction tasks and identify the fruitful features to consider. Guided by the exploratory analysis, the scientist needs to query the dataset to extract the relevant information and structure it in the standard tidy data (Dataframe) format so that cleaning and pre-processing tasks can be carried out by data science frameworks easily and efficiently \cite{tidydata}. The Dataframe abstraction bridges the gap between the data extraction and the data analysis tools and can be easily integrated with most of the state of the art analysis and machine learning tools that consume the input data as a Dataframe. The tidy data philosophy has been realized by the DataFrame data structure where each table represents one type of instances and each column represents an attribute of that type of instances and each row is values of the attributes for one instance.

% Paragraph 3: What is available now? now accessing it is done in \sparql which is hard to learn, has ambiguous semantics and still needs to be imported to analysis tools. 
Currenlty, there are two main RDF data extraction tools. First, the data is queried using \sparql, the standard query language for RDF data, by either loading the data into a local RDF engine and
querying it~\cite{rdf-3x} or by issuing the query  
to a public SPARQL endpoint hosting the data. The sparql query solution is then exported to one of the familiar tabular formats using scripts that deal with the scalability and communication issues manually. 
The second is writing ad-hoc scripts to process the graph and extract the required tabular information from it and importing it to the tidy data format. The first solution requires the user to learn a new query language, which involves a steep and time-consuming learning curve and requires manually importing the output of the data extraction tool \sparql to the data cleaning, processing, analysis and visualization tools that assume a tidy data format. The second solution requires substantial development effort and cannot be generalized to different graphs or even different information extracted from the same graph. In addition, it can be inefficient in the case of large graphs since all of the computation will be done by the script rather than an optimized query engine. Thus, both solutions look daunting as they both require much effort and time.

% we are suggesting RDFFrames: - the bridge between analysis tools and the querying tools 
% intoductory paragraph
In this paper, we propose RDFFrames which is an interface for querying and structuring RDF graphs to facilitate data science applications. RDFFrames allows the user to explore RDF graphs in a navigational way and extract information from them %(or simple algebric operators)
, join multiple datasets from multiple sources and apply simple filtering, grouping and aggregation operations using a simple procedural Python API that follows the split-apply-combine paradigm that is familiar to data scientists. RDFframes automatically converts the API calls made by the user to equivalent SPARQL queries and returns the answers in Dataframes. It facilitates data extraction by allowing easy and efficient navigation of knowledge graphs to build smaller datasets and allows combining information from multiple sources to exploit the heterogeniety of graph datasets. The API also allows for simple processing operations like filtering and aggregations and structures the data in the standard Dataframe format to make it much easier to apply cleaning and pre-processing tasks on the dataset. 


% What makes our API unique? - the integration with the data analysis tools - the easy querying API with the familiar procedural calls and the relational semantics -the scalability as we have lazy execution strategy and we handle the communication issues  
RDFFrames aims at exposing knowledge graphs to data science and machien learning tools in a scalable and efficient way.
To this end, RDFframes uses a lazy evaluation strategy that delays execution until the user asks for results.
Furthermore, the query generation algorithm translates the API calls into as few SPARQL queries as possible and pushes down as much computation as possible into the RDF engine or SPARQL endpoint.


% our contributions: - applying the tidy data principles to the RDF model. - proving the equivalence bwtween our relational operators and the sparql operators in the backend. % full python implementation
The main contributions of this paper are: 1) overcoming the impedance mismatch between knowledge graphs and data science tools by providing an API that queries and integrates RDF data with the familiar Dataframe model. 2) defining algebraic operators on the RDFFrames and proving their equivalence with some of the SPARQL operators. 

% outline of the paper
The rest of this paper is organized as follows: Section 2 describes the background on RDF fata model and SPARQL syntax and semantics. Section 3 defines the syntax of our operators. Section 4 describes the semantics of our operators. Section 5 demonstrates its applicability to data science tasks including both traditional machine learning and relational learning models. 